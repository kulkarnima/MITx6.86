{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b0b586c",
   "metadata": {},
   "source": [
    "## Digit recognition Demo\n",
    "\n",
    "### Intro\n",
    "Recognizing a handwritten digit is intuitive for the human eye, as we can effortlessly discern beween a scribbeled '7' and a rushed '2'. Yet, for machines, this is an intricate challenge. Imagine trying to teach a toddler to identify numbers, but with matrices of pixels, algorithms and various model architectures.In this demo, our journey begins with straightforward linear models. As we progress, we explore feed-forward neural networks, which use interconnected web of nodes to resemble how our brains process new information. Then, we navigate through convolutional neural networks (CNNs), designed to mimic the human visual system. With these tools in mind, we hope you can have a deeper understanding of neural networks and appreciate the power of machine learningalgorithms.\n",
    " \n",
    " For this demo, we will be using the MNIST dataset, which contains a vastcollection of handwritten digits that have been used in the machine learning community for years. Each image in this dataset is 28 × 28 pixels in size and represents a grayscale image of a single digit. So why should we care about classifying these images? Imagine a post office sorting through zip codes or a bank reading handwritten checks- having a machine to automatically recognize the digits with high accuracy can be very useful. The dataset contains 60,000 training samples and 10,00 testing samples, with a similar number of samples for each digit.\n",
    " \n",
    "#### Load data and Pre-Process\n",
    " We first load the required packages and the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c6bf2dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec0c78",
   "metadata": {},
   "source": [
    " Here, we load the MNIST dataset, which is split between training and testing split. More specifically, x_train is a (60000 x 28 x 28) numpy array of integers[0, 255], and y_train is a (60000, ) numpy array of labels [0, 10). To simplify our task, we consider binary classification where we zoom into two digits: 4 and 9. This choice allows us to understand the fundamental concepts of classification before tackling all ten digits. Let’s define some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6fcb5b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIG_A, DIG_B = 4, 9\n",
    "SIDE = 28\n",
    "MAX_PIX_VAL = 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab5078",
   "metadata": {},
   "source": [
    "and keep only the images of digits 4 and 9. To make our computations more stable and faster, we normalize our image data to have values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cde75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.logical_or((y_train == DIG_A), (y_train == DIG_B))\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]\n",
    "x_train = x_train/MAX_PIX_VAL\n",
    "N = len(x_train)\n",
    "\n",
    "assert(len(x_train) == len(y_train))\n",
    "'''\n",
    "assert(x_train.shape == (N,))\n",
    "'''\n",
    "assert(set(y_train) == {DIG_A, DIG_B})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf29023",
   "metadata": {},
   "source": [
    "It is crucial to randomize our dataset to ensure that the model does not accidentally learn any pattern from the order in which samples are presented. We also fix the seed to ensure code reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d11cea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "indices = np.arange(len(x_train))\n",
    "np.random.shuffle(indices)\n",
    "x_train = x_train[indices]\n",
    "y_train = y_train[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de6f2f0",
   "metadata": {},
   "source": [
    "Let us define the function close_enough to check if the data is appropriately normalized. Furthermore, given that there are roughly equal numbers of each digit in the dataset, we should have close to 12,000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a5b6410",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_enough = lambda a, b: abs(b-a) <= 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77a4c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(close_enough(np.min(x_train), 0.))\n",
    "assert(close_enough(np.max(x_train), 1.))\n",
    "assert(abs(N - 12000) < 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c44e59f",
   "metadata": {},
   "source": [
    "#### Evaluate Predictors\n",
    "Next, we define some functions to evaluate how good our model is using accuracy and cross-entropy loss. The judge function is a handy tool that lets us get both metrics for any predictors we pass in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "16819991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted_labels, true_ys):\n",
    "    return np.mean([1. if l==y else 0. for l, y in zip(predicted_labels, true_ys)])\n",
    "\n",
    "def cross_entropy_loss(predicted_probs, true_ys):\n",
    "    return np.mean([-np.log(p if y==DIG_B else 1.-p) for p, y in zip(predicted_probs, true_ys)])\n",
    "\n",
    "def judge(predictor, xs, ys):\n",
    "    probs = [predictor(x) for x in xs]\n",
    "    labels = [DIG_B if p > 0.5 else DIG_A for p in probs]\n",
    "    acc = accuracy(labels, ys)\n",
    "    loss = cross_entropy_loss(probs, ys)\n",
    "    return {'acc': acc, 'loss': loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433efec0",
   "metadata": {},
   "source": [
    "#### Dummy Predictors\n",
    "Before diving in to complicated predictiv emodels, let us define some dummy predictor functions that completely ignore the input image. They can either be very certain or uncertain about their prediction, or a fifty-fifty coin toss. These models, while naive in their predictions, give us a perspective on what’s achievable if we took a very basic approach to predicting our digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c833965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "very_sure_A = lambda x: 0.01\n",
    "very_sure_B = lambda x: 0.99\n",
    "maybe_its_A = lambda x: 0.4\n",
    "maybe_its_B = lambda x: 0.6\n",
    "fifty_fifty = lambda x: 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31113e6e",
   "metadata": {},
   "source": [
    "`very_sure_A` and `very_sure_B` arepredictors that are, as their name ssuggest,very sure about their predictions.The former always thinks the digitis ’A’ (or in our case, ’4’), while the latter is adamant it’s ’B’ (or’9’). These models don’t even look at the input image; they just confidently guess one digit. We can assert that the combined accuracy of the two models must sum up to one,because if one is correct, the other has to be incorrect, or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5a628df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa = judge(very_sure_A, x_train, y_train)['acc']\n",
    "vsb = judge(very_sure_B, x_train, y_train)['acc']\n",
    "assert(close_enough(vsa + vsb, 1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "cac80876",
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa = judge(very_sure_A, x_train[:1], [DIG_A])['acc']\n",
    "vsb = judge(very_sure_A, x_train[:1], [DIG_B])['acc']\n",
    "assert(close_enough(vsa, 1.))\n",
    "assert(close_enough(vsb, 0.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88871d17",
   "metadata": {},
   "source": [
    "Lastly,we examine the loss of these predictors, which gives us a measure of how off the predictors are from the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7d87cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa = judge(very_sure_A, x_train, y_train)['loss']\n",
    "vsb = judge(very_sure_B, x_train, y_train)['loss']\n",
    "mia = judge(maybe_its_A, x_train, y_train)['loss']\n",
    "mib = judge(maybe_its_B, x_train, y_train)['loss']\n",
    "ffl = judge(fifty_fifty, x_train, y_train)['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097f921",
   "metadata": {},
   "source": [
    "Cross-entropy loss quantifies the difference between two probability distributions, typically the true distribution and our model’s predictions. For the binary classification task, cross-entropy loss heavily penalizes a confident but incorrect prediction. Recall the formula for cross-entropy loss for binary classification:\n",
    "$$\n",
    " L =−y·log(p)−(1−y)·log(1−p)\n",
    " $$\n",
    " \n",
    "where $y$ is the true label, and p is the predicted probability of the label being 1. Therefore, for `very_sure_A`, the predictor encounters a near-zero loss for correct predictions, but a heavy penalty when making a confident but incorrect prediction. On the other hand, `maybe_its_A` makes predictions with lower confidence, thus encounters a more moderate loss for both correct and incorrect predictions.\n",
    " \n",
    " `fifty_fifty` always suggests that either digit is equally probable, thus the cross-entropy loss is simply $−log(1/2) = log2$. Given that the dataset contains similar number of samples for each label, `fifty_fifty` yields predictions that are closest to the baseline distribution. As the predictors become more extreme, each incorrect prediction yields heavier penalty that outweights a lower loss of correction prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fd19eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(ffl < mia < vsa)\n",
    "assert(ffl < mib < vsb)\n",
    "assert(close_enough(ffl, np.log(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011d422c",
   "metadata": {},
   "source": [
    "**Q:** what if the distribution of the two samples is skewed? For example, how will the cross-entropy loss relationship change if 90% of the samples are ’A’?\n",
    "\n",
    "#### Linear Models\n",
    "The journey of creating a linear model often starts with the activation function.In this case, we’re using the sigmoid function:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{(1+\\exp(-z)))}\n",
    "$$\n",
    "\n",
    "In order to ensure numerical stability, we clip the value of $z$ between −15 and 15 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d8beee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = lambda z: np.clip(z, -15, 15)\n",
    "sigmoid = lambda z: 1./(1.+np.exp(-clip(z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a3d64",
   "metadata": {},
   "source": [
    "With our activation function defined, let’s build our predictor! The linear prediction is simply the weighted sum of the input data passed through the sigmoid function. Here, we flatten the input into a 1D array, given that it is a 28 × 28 array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9dee7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_predict(w, x):\n",
    "    return sigmoid(w.dot(x.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fa59b",
   "metadata": {},
   "source": [
    "We initialize the w with random weights, similar to how a neural network is initialized. This is the linear mapping from the input. Here, we initialize $w$ with values drawn from a normal distribution normalized by diving the square root of the total number of pixels. This is to prevent any random weight from starting too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ed8b350",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(SIDE*SIDE) / np.sqrt(SIDE*SIDE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d7ff79",
   "metadata": {},
   "source": [
    "Let’s perform a sanity check on our linear implementation so far. Consider the predictor with our random weights $w$, we obtain a prediction based on the weighted sum of the values in $x$. If we feed the same sample, the weighted sum becomes the exact negative of the previous weighted sum because the weights are the negatives of the original weights. This will essentially mirror the input value of the sigmoid function from one side to the other. Since the sigmoid function is symmetric around the vertical line $z = 0$ (where $\\sigma(z) = 0.5$), if we get a value greater than 0.5 using $w$, we must get a value less than 0.5 using the negative weights $-w$, and vice versa. In other words, flipping the sign of $w$ guarantees that the two predictors will always make opposite predictions for any sample, and their respective confidence scores (the raw outputs of the sigmoid) will sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "816acb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vsa = judge(lambda x: linear_predict(+w, x), x_train, y_train)['acc']\n",
    "vsb = judge(lambda x: linear_predict(-w, x), x_train, y_train)['acc']\n",
    "assert(close_enough(vsa+vsb, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57dce1",
   "metadata": {},
   "source": [
    "We can also consider a special case of zero weights. In this case, all predictions have a 50-50 chance, and the associated loss must be approximately $log(2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f844d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = judge(lambda x: linear_predict(0*w, x), x_train, y_train)['loss']\n",
    "assert(close_enough(ffl, np.log(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9514061",
   "metadata": {},
   "source": [
    "Let’s do another interesting experiment. Suppose we create a random image $x$ using the random weights. In this case, the dot product between $w$ and $x$ will always be positive, thus the prediction will always be in favor of digit B. To verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "bca28123",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = w.reshape(SIDE, SIDE)\n",
    "vsa = judge(lambda x: linear_predict(w, x), [x], [DIG_A])['acc']\n",
    "vsb = judge(lambda x: linear_predict(w, x), [x], [DIG_B])['acc']\n",
    "assert(close_enough(vsa, 0.))\n",
    "assert(close_enough(vsb, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ca9705",
   "metadata": {},
   "source": [
    "#### Linear Backpropagation\n",
    "The term \"backpropagation\" is typically used for neural networks, but we can apply the same concept to linear models. In essence, it is the math behind adjusting the weights and learning from errors to get a better model. Consider the loss function\n",
    "$$\n",
    "L = CEL(\\sigma(w.x))\n",
    "$$\n",
    "\n",
    "where _CEL_ denots the cross-entropy loss and $\\sigma$ denotes the sigmoid function. Here, we use the chain rule to obtain the gradient of the loss function with respect to the weights. Let $z \\triangleq w.x$, and $p \\triangleq \\sigma(x)$, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial p}.\\frac{\\partial p}{\\partial z}.\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "\n",
    "Recall the cross-entropy loss for binary classications, we have:\n",
    "\n",
    "$$\n",
    "L(p, y) = \\begin{cases}\n",
    "            -log(p)   \\qquad\\qquad if\\quad y = \\mathrm{DIG\\_B} \\\\\n",
    "            -log(1-p) \\qquad  \\mathrm{otherwise}\n",
    "          \\end{cases}\n",
    "$$\n",
    "\n",
    "Taking the first derivative, we have:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p} = \n",
    "        \\begin{cases}\n",
    "            -1/p      \\quad\\qquad\\qquad if\\quad y = \\mathrm{DIG\\_B} \\\\\n",
    "            -1/(1-p)  \\quad\\qquad  \\mathrm{otherwise}\n",
    "        \\end{cases}\n",
    "$$        \n",
    "\n",
    "Next, we consider the derivate of signmoid (computation details ommitted).\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p}{\\partial z} = p(1-p)\n",
    "$$\n",
    "\n",
    "Finally, the derivative of $z$ with respect to $w$ is simply $x$ (flattened). Combining everything above, we have:\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial L}{\\partial p}.\\frac{\\partial p}{\\partial z}.\\frac{\\partial z}{\\partial w} =\n",
    "            \\begin{cases}\n",
    "                  (p-1).x  \\quad if\\quad y = \\mathrm{DIG\\_B} \\\\\n",
    "                   p.x     \\quad\\qquad \\mathrm{otherwise}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "Therefore, we can define the linear backpropogration function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "830aed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backprop(w, x, y):\n",
    "    z = w.dot(x.flatten())\n",
    "    p = sigmoid(z)\n",
    "    dl_dp = -(1 if y == DIG_B else -1) / (p if y == DIG_B else 1-p)\n",
    "    dp_dz = p * (1-p)\n",
    "    dz_dw = x.flatten()\n",
    "    \n",
    "    dl_dw = dl_dp * dp_dz * dz_dw\n",
    "    return dl_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ab59b",
   "metadata": {},
   "source": [
    "Let’s do a sanity check to make sure that the linear predictor is actually learning something. Using just one sample, we should expect the loss to decrease for each epoch. We also define a more generalized version of the function close_enough to compare two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2383356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_enough = lambda a, b: np.linalg.norm(np.array(b-a).flatten()) < 1e-6\n",
    "\n",
    "for _ in range(10):\n",
    "    w = np.random.randn(SIDE*SIDE) / np.sqrt(SIDE*SIDE)\n",
    "    x = x_train[0]\n",
    "    y = y_train[0]\n",
    "    \n",
    "    g = linear_backprop(w, x, y)\n",
    "    \n",
    "    before = judge(lambda xx: linear_predict(w, xx), [x], [y])['loss']\n",
    "    w = w - 0.01*g\n",
    "    after = judge(lambda xx: linear_predict(w, xx), [x], [y])['loss']\n",
    "    assert(after < before)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6b32cc",
   "metadata": {},
   "source": [
    " #### Building the SGD engine\n",
    " Next, we define a function that allows us to retrieve the next training example\n",
    " using a global index variable. We also initialize the random weights, define some\n",
    " learning parameters and number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1284bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "def next_training_example():\n",
    "    global idx\n",
    "    xy = x_train[idx], y_train[idx]\n",
    "    idx += 1\n",
    "    idx %= N\n",
    "    return xy\n",
    "\n",
    "w = np.random.randn(SIDE*SIDE) / np.sqrt(SIDE*SIDE)\n",
    "LEARNING_RATE = 0.1\n",
    "epochs = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec734f9",
   "metadata": {},
   "source": [
    "Now that everything is working, we can finally build our first SGD engine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "813ec741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at step      0tr acc0.50 tr loss 1.188\n",
      "at step   1000tr acc0.95 tr loss 0.137\n",
      "at step   2000tr acc0.95 tr loss 0.160\n",
      "at step   3000tr acc0.95 tr loss 0.143\n",
      "at step   4000tr acc0.95 tr loss 0.174\n",
      "at step   5000tr acc0.95 tr loss 0.151\n",
      "at step   6000tr acc0.97 tr loss 0.116\n",
      "at step   7000tr acc0.95 tr loss 0.209\n",
      "at step   8000tr acc0.94 tr loss 0.195\n",
      "at step   9000tr acc0.92 tr loss 0.289\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    x, y = next_training_example()\n",
    "    g = linear_backprop(w, x, y)\n",
    "    w = w - LEARNING_RATE * g\n",
    "    \n",
    "    if t % 1000:\n",
    "        continue\n",
    "        \n",
    "    ms = judge(lambda x: linear_predict(w, x), x_train, y_train)\n",
    "    print('at step {:6d}'.format(t) +\n",
    "          'tr acc{:4.2f} tr loss {:5.3f}'.format(ms['acc'], ms['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d164c8",
   "metadata": {},
   "source": [
    "Wetrain the linear predictor with a fixed learning rate for 10,000 epochs. For every 1,000 epoch, we evalute the model to obtain its training accuracy and loss. We get the following output (note that your results may differ due to randomness):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea104b0",
   "metadata": {},
   "source": [
    "As shown above, the model achieves a 96% accuracy! However, the model seems to reach its optimal performance in only 1,000 epochs. The loss indicates that the model is unstable, as it fluctuates between 0.11 and 0.17. This may be due to a relatively large learning rate, which prevents the training from stabilizing. Therefore, we can try a learning rate scheduler that decays the learning rate slowly overtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b7310625",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LEARNING_RATE * 1000. / (1000. + t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8976d46",
   "metadata": {},
   "source": [
    " Additionally, increase the number of epochs, and get the following output (again,\n",
    " your results may differ):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd64fe96",
   "metadata": {},
   "source": [
    "As shown above, we reached a lower loss at 0.087! From our exploration, it’s evident that even a linear model, despite its inherent simplicity, can exhibit profound capabilities in addressing complicated tasks like image classification, achieving a 97% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f7389",
   "metadata": {},
   "source": [
    "#### Vanilla Models\n",
    " Let’s move on to vanilla feed-forward neural networks! There are many high level deep learning frameworks (ie. PyTorch and TensorFlow) that cansimplify the following code in just a fewlines, but building the network froms cratch enables us to grasp the fundamentals. We first build the following network architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf57e5",
   "metadata": {},
   "source": [
    "```\n",
    "x\n",
    "h0-------->z1--->h1-------->z2--->h2-------->z3------>p\n",
    " |\n",
    " |\n",
    " |         |     |\n",
    " |         |     |           |     |\n",
    " | C*      |lrelu|     B*    |lrelu|    A*    |sigmoid|\n",
    " |         |     |           |     |  \n",
    " |         |     |           1\n",
    " |         1\n",
    " |\n",
    " D0        D1    D1          D2    D2         D3      1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a65e59",
   "metadata": {},
   "source": [
    "The inputs $x$ does through the network, layer by layer, to the output. Recall that the feed-forward neural network consists of one input layer, a fixed number of hidden layers, and one output layer. In our example, the networks consists fo two hideen layers, each equipped with leaky ReLU as it's activation function. Leaky ReLU is the modified version of ReLU which address the issue of vanishing gradients\n",
    "\n",
    "$$\n",
    "    \\mathrm{lrelu}(z) = \\mathrm{max}(z/10, z)\n",
    "$$\n",
    "\n",
    "The last row of the diagram indicates the dimensions of layer at each layer. Since our input is a 28x28 image, the input to the network is a flattened array of 784 dimension. For simplicity we set D1 = D2 = 32, which is more so an arbritary choice. In practice, the number of neurons in hidden layers is often chosen to be power of 2, which can result in computational efficiency due to how hardware architectures like GPUs are designed. Notice the \"1\" below each layer? That is the bias term added to each layer. Now, we build this vanilla network and initialize it with random weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "447c5cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "D0 = SIDE*SIDE\n",
    "D1, D2, D3 = 32, 32, 1\n",
    "\n",
    "def vanilla_init():\n",
    "    A = np.random.randn(    D2) / np.sqrt( 1+D2)\n",
    "    B = np.random.randn(D2, D1) / np.sqrt(D2+D1)\n",
    "    C = np.random.randn(D1, D0) / np.sqrt(D1+D0)\n",
    "    \n",
    "    return (A, B, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb6e2e",
   "metadata": {},
   "source": [
    "Theweights are initialized using the Xavier initialization, which scales the weights by the inverse of the square root of the sum of the input and output sizes. This helps to achieve a variance of activations that remain the same across layers, facilitating better convergence during training. Next, we define the activation function and the feed-forward prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b10e66a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrelu = lambda z: np.maximum(z/10, z)\n",
    "\n",
    "def vanilla_predict(abc, x):\n",
    "    A, B, C = abc\n",
    "    \n",
    "    h0 = x.flatten()\n",
    "    z1 = C.dot(h0)\n",
    "    h1 = lrelu(z1)\n",
    "    \n",
    "    z2 = B.dot(h1)\n",
    "    h2 = lrelu(z2)\n",
    "    \n",
    "    z3 = A.dot(h2)\n",
    "    p = sigmoid(z3)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909fdd5",
   "metadata": {},
   "source": [
    "This function passes the input x through the network, layer by layer, with the sigmoid function at the end to ensure that the output is between 0 and 1, indicating the prediction probability. Similarly, we perform a sanity check on a dummy predictor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7fd1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B, C = vanilla_init()\n",
    "vsa = judge(lambda x: vanilla_predict((+A, B, C), x), x_train, y_train)['acc']\n",
    "vsb = judge(lambda x: vanilla_predict((-A, B, C), x), x_train, y_train)['acc']\n",
    "assert(close_enough(vsa+vsb, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc18880",
   "metadata": {},
   "source": [
    "Here, vsa computes the accuracy using the initial random weights, whereas vsb computes the accuracy when the sign of the weights of the output layer is flipped, essentially leading to opposite predictions for the same input. Therefore, the accuracy of the two predictors must sum up to one. Next, consider another predictor where A is set to zero. In this case, the output layer’s activation are forced to be near 0.5, turning the model into a fifty-fifty predictor. Recall that the loss for this type of predictor should have loss of log2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21cf1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffl = judge(lambda x: vanilla_predict((0*A, B, C), x), x_train, y_train)['loss']\n",
    "assert(close_enough(ffl, np.log(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded4b21",
   "metadata": {},
   "source": [
    "To further understand the effects of these weights, we can perform an interesting experiment. Suppose we force all the weights to be positive, then the values in each neuron would be positive, leading to a prediction that always favors DIG_B. We can verify this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "07957db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_train[0]\n",
    "y = y_train[0]\n",
    "A = np.abs(A)\n",
    "B = np.abs(B) \n",
    "C = np.abs(C)\n",
    "\n",
    "acc_ppp = judge(lambda x: vanilla_predict((A, B, C), x), [x], [DIG_B])['acc']\n",
    "assert(close_enough(acc_ppp, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321e792d",
   "metadata": {},
   "source": [
    "Suppose we flip one of the three matrices, such that all entries in the matrix are negative. Therefore, the inputs and outputs of that particular layer would be opposite, leading to a prediction that always favors DIG_A. We can verify this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ab6c5196",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ppn = judge(lambda x: vanilla_predict((A, B, -C), x), [x], [DIG_B])['acc']\n",
    "acc_pnp = judge(lambda x: vanilla_predict((A, -B, C), x), [x], [DIG_B])['acc']\n",
    "assert close_enough(acc_ppn, 0.)\n",
    "assert close_enough(acc_pnp, 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d30629",
   "metadata": {},
   "source": [
    "Similarly, if we flip two of the matrices, the two negative weights will end up negating each other’s effect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "59c81fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_pnn = judge(lambda x: vanilla_predict((A,-B,-C), x), [x], [DIG_B])['acc']\n",
    "assert(close_enough(acc_pnn, 1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a967879",
   "metadata": {},
   "source": [
    "####  Vanilla Backpropagation\n",
    "Now, we are ready to implement the back-propagation for the vanilla model. Recall that the leaky ReLU is defined as:\n",
    "\n",
    "$$\n",
    "\\mathrm{lrelu}(z) = \\mathrm{max}(z/10, z) = \\begin{cases}\n",
    "    z     \\qquad \\mathrm{if}\\quad z \\gt 0 \\\\\n",
    "    z/10  \\quad \\mathrm{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We compute it's gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial lrelu}{\\partial z} = \\begin{cases}\n",
    "    1     \\qquad \\mathrm{if}\\quad z \\gt 0 \\\\\n",
    "    0.1   \\quad  \\mathrm{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Observe that leaky ReLU is not diferentiable at $z = 0$, although it is typically defined to be the same as the negative side to avoid undefined values. In this example, we define it to be the average of the two derivatives (ie. 0.55). Note that in practice, the gradient at z = 0 does not really matter due to floating-point arithmetic in Python. The probability of getting an exact value of 0 is extremely small. Therefore, we can define the derivative as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f7eb841c",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = lambda z: np.heaviside(z, 0.5)\n",
    "dlrelu_dz = lambda z: 0.1 + (1. - 0.1)*step(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07f934",
   "metadata": {},
   "source": [
    "Note that:\n",
    "\n",
    "$$\n",
    "\\mathrm{np.heaviside}(z, c) = \\begin{cases}\n",
    "    0 \\qquad \\mathrm{if} z \\lt 0 \\\\\n",
    "    c \\qquad \\mathrm{if} z = 0 \\\\\n",
    "    1 \\qquad \\mathrm{if} z \\gt 0 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Consider the weights $abc = (A, B, C)$, and give sample $x$ with label $y$. For each update, the function first performs forward pass through the network to compute the probability of class DIG_B of the given network for $x$.\n",
    "\n",
    "Next, the function computes the gradients of the loss with respect to the parameters by applying the chain rule backward through the network, from the output layer to the input layer. This part calculates how much each weight contributed to the error in the output and thus, how much the weights should be adjusted. Here, np.outer computes the gradient for each weight in the layer based on how much changing that specific weight would affect the loss. The resulting matrix of the outer product has the same shape as the weight matrix, and each element of the result represents the partial derivative of the loss with respect to the corresponding weight in the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8b118f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_backprop(abc, x, y):\n",
    "    A, B, C = abc\n",
    "    \n",
    "    h0 = x.flatten()\n",
    "    z1 = C.dot(h0)\n",
    "    h1 = lrelu(z1)\n",
    "    \n",
    "    z2 = B.dot(h1)\n",
    "    h2 = lrelu(z2)\n",
    "    \n",
    "    z3 = A.dot(h2)\n",
    "    p = sigmoid(z3)\n",
    "    \n",
    "    dl_dz3 = p - (1 if y == DIG_B else 0)\n",
    "    dl_dh2 = dl_dz3 * A\n",
    "    dl_dz2 = dl_dh2 * dlrelu_dz(z2)\n",
    "    dl_dh1 = dl_dz2.dot(B)\n",
    "    dl_dz1 = dl_dh1 * dlrelu_dz(z1)\n",
    "    \n",
    "    dl_dA = dl_dz3 * h2\n",
    "    dl_dB = np.outer(dl_dz2, h1)\n",
    "    dl_dC = np.outer(dl_dz1, h0)\n",
    "    \n",
    "    return dl_dA, dl_dB, dl_dC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d94f85",
   "metadata": {},
   "source": [
    "Next, we define a displacement function that updates the parameters of the network. For each parameter, the corresponding gradient is multiplied by the learning rate then added to the current parameter value, performing a gradient descent update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0f1e9e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_displace(abc, coef, g):\n",
    "    A, B, C = abc\n",
    "    gA, gB, gC = g\n",
    "    return (A + coef * gA,\n",
    "            B + coef * gB,\n",
    "            C + coef * gC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdae418",
   "metadata": {},
   "source": [
    " Combining everything above, let’s train the vanilla network!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9b895fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at step      0tr acc 0.6115 tr loss 0.685\n",
      "at step   1000tr acc 0.4955 tr loss   nan\n",
      "at step   2000tr acc 0.4955 tr loss   nan\n",
      "at step   3000tr acc 0.4955 tr loss   nan\n",
      "at step   4000tr acc 0.4955 tr loss   nan\n",
      "at step   5000tr acc 0.4955 tr loss   nan\n",
      "at step   6000tr acc 0.4955 tr loss   nan\n",
      "at step   7000tr acc 0.4955 tr loss   nan\n",
      "at step   8000tr acc 0.4955 tr loss   nan\n",
      "at step   9000tr acc 0.4955 tr loss   nan\n",
      "at step  10000tr acc 0.4955 tr loss   nan\n",
      "at step  11000tr acc 0.4955 tr loss   nan\n",
      "at step  12000tr acc 0.4955 tr loss   nan\n",
      "at step  13000tr acc 0.4955 tr loss   nan\n",
      "at step  14000tr acc 0.4955 tr loss   nan\n",
      "at step  15000tr acc 0.4955 tr loss   nan\n"
     ]
    }
   ],
   "source": [
    "abc = vanilla_init()\n",
    "T = 1000\n",
    "''' \n",
    "parameters for momentum \n",
    " \n",
    "m = (0, 0, 0)\n",
    "beta = 0.9\n",
    "'''\n",
    "for t in range(T*15+1):\n",
    "    x, y = next_training_example()\n",
    "    g = vanilla_backprop(abc, x, y)\n",
    "    LR = LEARNING_RATE * 4000. / (4000 + t)\n",
    "\n",
    "    '''\n",
    "    Uncomment this line and comment lines 16-18 for eliminating momentum\n",
    "    m = vanilla_displace(m, beta-1, m)\n",
    "    m = vanilla_displace(m, 1, g)\n",
    "    abc = vanilla_displace(abc, -LR, m)\n",
    "    '''\n",
    "    \n",
    "    abc = vanilla_displace(abc, -LR, g)\n",
    "\n",
    "    if t % 1000:\n",
    "        continue\n",
    "        \n",
    "    ms = judge(lambda x: vanilla_predict(abc, x), x_train, y_train)\n",
    "    print('at step {:6d}'.format(t) +\n",
    "          'tr acc {:4.4f} tr loss {:5.3f}'.format(ms['acc'], ms['loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0bd4ac",
   "metadata": {},
   "source": [
    " This indicates that the network reaches a training loss of around 0.05, with an 98 −99% accuracy! Additionally, we can implement the idea of momentum, which also considers gradients from previous updates. We first initialize the momentum to zero after we initialize the weights of the model (at line 2): `m = vanilla_displace(abc,-1., abc)`. This is a hacky way to initialize momentum with the same shape as model parameters. Next, we change the model updates to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "72c14467",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 0.9\n",
    "\n",
    "m = vanilla_displace(m, beta-1, m)\n",
    "m = vanilla_displace(m, 1, g)\n",
    "abc = vanilla_displace(abc, -LR, m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41e4ba",
   "metadata": {},
   "source": [
    "The parameter beta is the momentum coefficient, which controls how much of the previous gradients to be considered. For example, when β = 0.9, it means 90% of the previous velocity is combined with 10% of the current gradient to update the parameters. Line 2 indicates that we are “forgetting” 10% of the previous velocity (thus 90% remaining), then we the current gradient to the velocity. Then we update the parameters based on the velocity. In practice, we typically\n",
    " set the momentum parameter to some value between 0.8 and 0.99. Using this technique, we can sometimes result in faster convergence and higher accuracy (although the effects are not as profound in this example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec24ce55",
   "metadata": {},
   "source": [
    " #### Building a CNN\n",
    " For the last model, we will be building a convolutional neural network (CNN). A CNN is a specialized type of neural network designed for processing grid structured input data such as images, where spatial hierarchies and local patterns are crucial. Unlike fully-connected neural networks where each neuron in one layer is connected to every neuron in the next layer, CNNs use convolutional layers to preserve the spatial relationship between pixels by learning image features using small squares of input data. We define the architecture as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f125dddb",
   "metadata": {},
   "source": [
    "```                     \n",
    "                        height x width x channels\n",
    "    \n",
    "x                          28 x 28 x 1\n",
    "           avgpool                                           2 x 2\n",
    "h0                         14 x 14 x 1\n",
    "           conv                                 weight C     5 x 5 x 8 x 1   stride 2 x 2\n",
    "z1                           5 x 5 x 8      \n",
    "           lrelu           \n",
    "h1                           5 x 5 x 8\n",
    "           conv                                 weight B     1 x 1 x 4 x 8   stride 1 x 1\n",
    "z2                           5 x 5 x 4\n",
    "           lrelu \n",
    "h2                           5 x 5 x 4\n",
    "           dense                                weight A     1 x (5 x 5 x 4)\n",
    "z3                                   1\n",
    "           sigmoid\n",
    "p                                    1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4991a7",
   "metadata": {},
   "source": [
    "Let's break down the architecture layer, from top to bottom:\n",
    "1. input x: the input is an image with dimensions 28 x 28 with 1 channel, since it is a grayscale image\n",
    "2. avgpool: this layer performans average pooling with 2 x 2 windows, effectively reducing the spatial dimensions by a factor of 2. After poolng, the dimension becomes 14 x 14 x 1\n",
    "3. conv: the first convolution layer has filters of size 5 x 5, 8 output channels with a 2 x 2 stride. Given the input size of 14 x 14, the feature map size becomes\n",
    "$$\n",
    "\\frac{14 - 5}{2} + 1 = 5\n",
    "$$\n",
    "The output has dimension 5 x 5 x 8. Leaky ReLU does not change the dimensions.\n",
    "4. The second convolution layer has filters of size 1 x 1, 4 output channels and operating on 8 input channels with a stride of 1 x 1. The spatial diemsnions remain the same, but the number of channels changes to 4. The output has dimension 5 x 5 x 4. Leaky ReLU does not change the dimensions.\n",
    "5. We use a fully-connected layer that reduces the dimensions to a singlevalue, followed by a sigmoid activation function which squashes the outpu tbetween 0 and 1, which is what we need for binary classification tasks.\n",
    "\n",
    "Now, we define the functions for average pooling and convolution layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d31b7b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgpool2x2(x):\n",
    "    H, W, C = x.shape\n",
    "    return ( x[0:H:2, 0:W:2]\n",
    "            +x[0:H:2, 1:W:2]\n",
    "            +x[1:H:2, 0:W:2]\n",
    "            +x[1:H:2, 1:W:2])/4\n",
    "\n",
    "def conv(x, weights, stride=1):\n",
    "    H, W, C = x.shape\n",
    "    KH, KW, OD, ID = weights.shape\n",
    "    \n",
    "    assert(C == ID)\n",
    "    \n",
    "    HH, WW = int((H - KH + 1)/stride), int((W - KW + 1)/stride)\n",
    "    \n",
    "    return np.array(\n",
    "        [[\n",
    "            np.tensordot(\n",
    "                weights,\n",
    "                x[h:h+KH, w:w+KW],\n",
    "                ((0, 1, 3), (0, 1, 2))\n",
    "            )\n",
    "            for w in range(0, WW*stride, stride)]\n",
    "        for h in range(0, HH*stride, stride)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f96c1a",
   "metadata": {},
   "source": [
    "The function `avgpool2x2` performs 2x2 pooling, which means it averages the values of 2x2 adjacent pixels and produces a pooled output, hence reducing the spatial dimensions by a factor of 2. The function `conv` performs convolution operation between the input x and weights with a specified stride. The assertion test ensures that the number of channels in the input is the same as the number of channels to the input depth of the weights ID. The returned value includes the dot products between the weights and the respective regions of the input tensor, considering the stride. It iterates over the height and width of the possible positions of the filter on the input tensor and calculates the corresponding output for each position, eventually creating an array representing the output feature map. Now, we perform some sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6fc90858",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.ones((8, 12, 7))\n",
    "pp = np.ones((4, 6, 7))\n",
    "assert close_enough(avgpool2x2(aa), pp)\n",
    "ww = np.ones((3, 3, 5, 7))\n",
    "cc = (3*3*7)*np.ones((6, 10, 5))\n",
    "assert close_enough(conv(aa, ww, stride=1), cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd09dbc",
   "metadata": {},
   "source": [
    "We first initialize `aa` as an array of all ones with shape 8 x 12 x 7 (height `x`width `x` channel). When we perform a 2x2 average pooling on it, we effectively shrink the dimensions by a factor of 2. Since all values are ones, the average value in each pooling region is also one. Next, `ww` represents the weights of the convolutional layer and has dimensions 3 x 3 x 5 x 7 (kernel height `x` kernel width `x` output channels `x` input channels). We perform a convolution operation on `aa` with weights `ww` and stride of 1. The output dimensions can be calculated as (8−3+1,12−3+1) = (6,10), thus matching the dimensions of\n",
    " `cc`. Since the weights `ww` are all ones, each value in the output `cc` should be the sum of 3 x 3 x 7 ones from the input `aa`, hence each value in `cc` should be 3·3·7 = 63. These two sanity checks test the scaling and shapes of the functions above. Now, we test if the results match in terms of their orientations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "71f473f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = np.array([1 * np.eye(4), 3 * np.eye(4)])\n",
    "pp = np.array([[[1, 1, 0, 0], [0, 0, 1, 1]]])\n",
    "assert close_enough(avgpool2x2(bb), pp)\n",
    "ww = np.zeros([2, 2, 1, 4])\n",
    "ww[0, 0, :, :] = 1 + np.arange(4)\n",
    "cc = np.array([1, 2, 3])[np.newaxis, :, np.newaxis]\n",
    "assert close_enough(conv(bb, ww, stride=1), cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621fc8f",
   "metadata": {},
   "source": [
    "`bb` is equivalent to two 4 x 4 identity matrices stacked together, with the second scaled by 3. When avgpool2x2 is applied to `bb`, it performs average pooling over 2 x 2non-overlapping windows. Each channel is an input of dimension 2 x 4, in which the i’th column of the i’th channel being 1 and 3, e.g.\n",
    "\n",
    "$$\n",
    "C1 = {\\left\\lbrack \\matrix{1 & 0 & 0 & 0 \\cr 3 & 0 & 0 & 0} \\right \\rbrack}\n",
    "$$\n",
    "\n",
    " After average pooling with 2 x 2 windows, we have:\n",
    " \n",
    " $$\n",
    " C1 = C2 = {\\left\\lbrack \\matrix{1 & 0}\\right \\rbrack}, C3 = C4 = {\\left\\lbrack \\matrix{0 & 1}\\right \\rbrack}\n",
    " $$\n",
    " \n",
    "Stacking the channels together, we yield an array with dimensions 1 x 2 x 4:\n",
    "\n",
    "$$\n",
    "pp = {\\left\\lbrack \\matrix{1 & 1 & 0 & 0 \\cr 0 & 0 & 1 & 1} \\right \\rbrack}\n",
    "$$\n",
    "\n",
    "For the second sanity check, `w` represents the weights of the convolution layer with dimension 2 x 2 x 1 x 4(kernel height x kernel width x output channels x input channels). Observe that only the top-left values of the weights are non-zero. Therefore, the convolution for each channel would be the sum of the diagonal of `bb` scaled by the corresponding weight in `ww[0, 0]`, which results in the array `cc`.\n",
    "\n",
    "Now, we compute the gradients of the convolution layer. We write two helper functions that, when given the outputs to the convolution layer, give us the gradients with respect to the weights or the inputs to the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "700e8837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dw_conv(x, weights_shape, dl_dconv, stride=1):\n",
    "    H, W, C = x.shape\n",
    "    KH, KW, OD, ID = weights_shape\n",
    "    assert C == ID\n",
    "    HH, WW = int((H-KH+1)/stride), int((W-KW+1)/stride)\n",
    "    assert dl_dconv.shape == (HH, WW, OD)\n",
    "    \n",
    "    HS, WS = HH*stride, WW*stride\n",
    "    dl_dw = np.array(\n",
    "        [[\n",
    "            np.tensordot(\n",
    "                dl_dconv,\n",
    "                x[dh:dh+HS:stride, dw:dw+WS:stride],\n",
    "                ((0, 1), (0, 1))\n",
    "            )\n",
    "            for dw in range(HW)]\n",
    "        for dh in range(KH)]\n",
    "    )\n",
    "    return dl_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e147050",
   "metadata": {},
   "source": [
    " This function gives us the derivative of the loss with respect to the weights of the\n",
    " convolution layer. Each entry of the derivative:\n",
    " \n",
    " $$\n",
    " \\frac{\\partial L}{\\partial W_{ij}} = \\mathrm{np.tensordot}(\\frac{\\partial L}{\\partial C_{ij}}, x_{sliced}, ((0, 1), (0, 1)))\n",
    " $$\n",
    " \n",
    " computes the sum of element-wise products of the gradients `dl_dconv` and the\n",
    " corresponding input values `x_sliced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e38b381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dx_conv(x_shape, weights, dl_conv, stride):\n",
    "    H, W, C = x_shape\n",
    "    KH, KW, OD, ID = weights.shape\n",
    "    assert C == ID\n",
    "    HH, WW = int((H-KH+1)/stride), int((W-KW+1)/stride)\n",
    "    assert dl_dconv.shape == (HH, WW, OD)\n",
    "    \n",
    "    dl_dx = np.zeros((H, W, ID), dtype=np.float32)\n",
    "    for h in range(KH):\n",
    "        for w in range(KW):\n",
    "            dl_dx[h:h+HH*stride:stride, w:w+WW*stride:stride] += (\n",
    "                np.tensordot(\n",
    "                    dl_dconv,\n",
    "                    weights[h, w],\n",
    "                    ((2, ), (0, ))\n",
    "                )\n",
    "            )\n",
    "    return dl_dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9677c4",
   "metadata": {},
   "source": [
    " The second function gives us the derivative of the loss with respect to the inputs\n",
    " to the convolution layer. Each entry:\n",
    " \n",
    " $$\n",
    " \\frac{\\partial L}{\\partial x_{ij}} = \\sum_{h, w} \\frac{\\partial L}{\\partial C_{i-h, j-w}}.W_{hw}\n",
    " $$\n",
    " \n",
    " is the sum over all positions in the kernel, and accumulates the contribution to the gradient at each input location `(i,j)` due to each weight in the kernel. With these building blocks, let’s build the full back-propagation for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436afbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backprop(abc, x, y):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
